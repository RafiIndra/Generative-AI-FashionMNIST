{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Reshape, BatchNormalization, LeakyReLU, UpSampling2D, Conv2D, ReLU, Flatten, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "OkgTuoPwbMb5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to normalize images\n",
        "def normalize_image(image, mean=0.5, std=0.5):\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)  # Converts [0, 255] to [0, 1]\n",
        "    image = (image - mean) / std  # Normalize to [-1, 1]\n",
        "    return image\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_data(image, label):\n",
        "    image = normalize_image(image)\n",
        "    return image, label\n",
        "\n",
        "# Example with MNIST dataset\n",
        "dataset = tfds.load('FashionMNIST', split='train', as_supervised=True, batch_size=32)\n",
        "\n",
        "# Apply preprocessing\n",
        "ds1 = dataset.map(preprocess_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Set up DataLoader\n",
        "batch_size = 32\n",
        "# dataloader = dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "UDF4vr3uE7HH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tfds.load('FashionMNIST', split='train', shuffle_files=True)\n",
        "ds1 = ds.batch(32, drop_remainder=True)"
      ],
      "metadata": {
        "id": "3G6khUnCHVlz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ds1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBomt5b6afEx",
        "outputId": "a4bba632-f2f5-46c0-b301-e839010e4883"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1875"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "generator = Sequential()\n",
        "\n",
        "# Input layer\n",
        "generator.add(Dense(128 * 7 * 7, input_dim=100))\n",
        "generator.add(ReLU())\n",
        "generator.add(Reshape((7, 7, 128)))\n",
        "\n",
        "# Upsampling layers\n",
        "generator.add(UpSampling2D(size=(2, 2)))\n",
        "generator.add(Conv2D(128, kernel_size=(3, 3), padding='same'))\n",
        "generator.add(BatchNormalization(momentum=0.78))\n",
        "generator.add(ReLU())\n",
        "\n",
        "generator.add(UpSampling2D(size=(2, 2)))\n",
        "generator.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
        "generator.add(BatchNormalization(momentum=0.78))\n",
        "generator.add(ReLU())\n",
        "\n",
        "# Output layer\n",
        "generator.add(Conv2D(1, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
        "\n",
        "# Display the generator architecture\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5aHlpHVI917",
        "outputId": "70a626b2-41aa-41c6-c269-1274b0b1c73b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 6272)              633472    \n",
            "                                                                 \n",
            " re_lu_10 (ReLU)             (None, 6272)              0         \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " up_sampling2d_7 (UpSamplin  (None, 14, 14, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 14, 14, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (None, 14, 14, 128)       512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_11 (ReLU)             (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " up_sampling2d_8 (UpSamplin  (None, 28, 28, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 28, 28, 64)        73792     \n",
            "                                                                 \n",
            " batch_normalization_14 (Ba  (None, 28, 28, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " re_lu_12 (ReLU)             (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 28, 28, 1)         577       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 856193 (3.27 MB)\n",
            "Trainable params: 855809 (3.26 MB)\n",
            "Non-trainable params: 384 (1.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Sequential()\n",
        "\n",
        "# Input layer\n",
        "discriminator.add(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same', input_shape=(28, 28, 1)))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "# Hidden layers\n",
        "discriminator.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same'))\n",
        "discriminator.add(BatchNormalization(momentum=0.82))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "discriminator.add(Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same'))\n",
        "discriminator.add(BatchNormalization(momentum=0.82))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.25))\n",
        "\n",
        "discriminator.add(Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same'))\n",
        "discriminator.add(BatchNormalization(momentum=0.8))\n",
        "discriminator.add(LeakyReLU(alpha=0.25))\n",
        "discriminator.add(Dropout(0.25))\n",
        "\n",
        "discriminator.add(Flatten())\n",
        "\n",
        "# Output layer\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Display the discriminator architecture\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouVGEF6wa9NK",
        "outputId": "2e34cc79-c4a8-4a31-efc7-452714ef1cbf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_23 (Conv2D)          (None, 14, 14, 32)        320       \n",
            "                                                                 \n",
            " leaky_re_lu_10 (LeakyReLU)  (None, 14, 14, 32)        0         \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 7, 7, 64)          18496     \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 7, 7, 64)          256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " leaky_re_lu_11 (LeakyReLU)  (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 4, 4, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " leaky_re_lu_12 (LeakyReLU)  (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 2, 2, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_17 (Ba  (None, 2, 2, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " leaky_re_lu_13 (LeakyReLU)  (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 390657 (1.49 MB)\n",
            "Trainable params: 389761 (1.49 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set the number of epochs and batch size for training\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "optimizer_G = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "optimizer_D = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "# Loss function\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return (real_loss + fake_loss) / 2\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "# Set the label for real and fake images\n",
        "real_labels = np.ones((batch_size, 1))\n",
        "fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  i=0\n",
        "  for batch in ds1:\n",
        "    # Generate a batch of fake images\n",
        "    z = np.random.normal(0,1,(batch_size, latent_dim))\n",
        "    with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n",
        "      fake_images = generator(z, training=True)\n",
        "\n",
        "      real = discriminator(batch[0], training=True)\n",
        "      fake = discriminator(fake_images, training=True)\n",
        "\n",
        "      d_loss = discriminator_loss(real, fake)\n",
        "      g_loss = generator_loss(fake)\n",
        "\n",
        "    d_grad = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "    g_grad = g_tape.gradient(g_loss, generator.trainable_variables)\n",
        "\n",
        "    # Update weights\n",
        "    optimizer_D.apply_gradients(zip(d_grad, discriminator.trainable_variables))\n",
        "    optimizer_G.apply_gradients(zip(g_grad, generator.trainable_variables))\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "      print(f\"Epoch [{epoch+1}/{epochs}]\\\n",
        "      Batch [{i+1}/{len(ds1)}] DLoss: {d_loss.numpy():.4f} GLoss: {g_loss.numpy():.4f}\")\n",
        "    i+=1\n",
        "\n",
        "# Save the trained generator model\n",
        "generator.save('/path/to/trained_generator.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rdu4C-3giA-",
        "outputId": "b1453f97-932f-4d1b-ad27-158a1144083d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]      Batch [100/1875] DLoss: 0.2989 GLoss: 1.5956\n",
            "Epoch [1/10]      Batch [200/1875] DLoss: 0.3999 GLoss: 2.4884\n",
            "Epoch [1/10]      Batch [300/1875] DLoss: 0.1903 GLoss: 2.8175\n",
            "Epoch [1/10]      Batch [400/1875] DLoss: 0.1946 GLoss: 3.0018\n",
            "Epoch [1/10]      Batch [500/1875] DLoss: 0.1692 GLoss: 2.5983\n",
            "Epoch [1/10]      Batch [600/1875] DLoss: 0.2664 GLoss: 2.4058\n",
            "Epoch [1/10]      Batch [700/1875] DLoss: 0.2806 GLoss: 2.5407\n",
            "Epoch [1/10]      Batch [800/1875] DLoss: 0.8667 GLoss: 0.4151\n",
            "Epoch [1/10]      Batch [900/1875] DLoss: 0.4626 GLoss: 1.9596\n",
            "Epoch [1/10]      Batch [1000/1875] DLoss: 0.3846 GLoss: 1.4661\n",
            "Epoch [1/10]      Batch [1100/1875] DLoss: 0.2867 GLoss: 2.2606\n",
            "Epoch [1/10]      Batch [1200/1875] DLoss: 0.6720 GLoss: 1.0748\n",
            "Epoch [1/10]      Batch [1300/1875] DLoss: 0.3515 GLoss: 2.2755\n",
            "Epoch [1/10]      Batch [1400/1875] DLoss: 0.2380 GLoss: 1.5684\n",
            "Epoch [1/10]      Batch [1500/1875] DLoss: 0.4685 GLoss: 3.2156\n",
            "Epoch [1/10]      Batch [1600/1875] DLoss: 0.5867 GLoss: 1.9382\n",
            "Epoch [1/10]      Batch [1700/1875] DLoss: 0.2527 GLoss: 2.0493\n",
            "Epoch [1/10]      Batch [1800/1875] DLoss: 0.2271 GLoss: 1.7180\n",
            "Epoch [2/10]      Batch [100/1875] DLoss: 0.2552 GLoss: 2.4461\n",
            "Epoch [2/10]      Batch [200/1875] DLoss: 0.3945 GLoss: 1.3111\n",
            "Epoch [2/10]      Batch [300/1875] DLoss: 0.2072 GLoss: 1.9234\n",
            "Epoch [2/10]      Batch [400/1875] DLoss: 0.4366 GLoss: 1.7067\n",
            "Epoch [2/10]      Batch [500/1875] DLoss: 0.2112 GLoss: 1.7560\n",
            "Epoch [2/10]      Batch [600/1875] DLoss: 0.3530 GLoss: 2.5186\n",
            "Epoch [2/10]      Batch [700/1875] DLoss: 0.5462 GLoss: 0.8774\n",
            "Epoch [2/10]      Batch [800/1875] DLoss: 0.1696 GLoss: 2.3045\n",
            "Epoch [2/10]      Batch [900/1875] DLoss: 0.6409 GLoss: 0.5688\n",
            "Epoch [2/10]      Batch [1000/1875] DLoss: 0.3643 GLoss: 1.1450\n",
            "Epoch [2/10]      Batch [1100/1875] DLoss: 0.3203 GLoss: 1.3317\n",
            "Epoch [2/10]      Batch [1200/1875] DLoss: 0.4284 GLoss: 1.4969\n",
            "Epoch [2/10]      Batch [1300/1875] DLoss: 0.4727 GLoss: 1.6172\n",
            "Epoch [2/10]      Batch [1400/1875] DLoss: 0.4782 GLoss: 2.7453\n",
            "Epoch [2/10]      Batch [1500/1875] DLoss: 0.2305 GLoss: 2.2838\n",
            "Epoch [2/10]      Batch [1600/1875] DLoss: 0.5705 GLoss: 2.1136\n",
            "Epoch [2/10]      Batch [1700/1875] DLoss: 0.2520 GLoss: 2.9375\n",
            "Epoch [2/10]      Batch [1800/1875] DLoss: 0.1475 GLoss: 3.0261\n",
            "Epoch [3/10]      Batch [100/1875] DLoss: 0.2324 GLoss: 3.0166\n",
            "Epoch [3/10]      Batch [200/1875] DLoss: 0.3157 GLoss: 4.1919\n",
            "Epoch [3/10]      Batch [300/1875] DLoss: 0.4061 GLoss: 1.0725\n",
            "Epoch [3/10]      Batch [400/1875] DLoss: 0.2652 GLoss: 2.9311\n",
            "Epoch [3/10]      Batch [500/1875] DLoss: 0.2288 GLoss: 2.5551\n",
            "Epoch [3/10]      Batch [600/1875] DLoss: 0.2525 GLoss: 3.4347\n",
            "Epoch [3/10]      Batch [700/1875] DLoss: 0.6985 GLoss: 2.9658\n",
            "Epoch [3/10]      Batch [800/1875] DLoss: 0.3790 GLoss: 1.0271\n",
            "Epoch [3/10]      Batch [900/1875] DLoss: 0.3125 GLoss: 1.5249\n",
            "Epoch [3/10]      Batch [1000/1875] DLoss: 0.5984 GLoss: 1.5251\n",
            "Epoch [3/10]      Batch [1100/1875] DLoss: 0.5257 GLoss: 0.7554\n",
            "Epoch [3/10]      Batch [1200/1875] DLoss: 0.2135 GLoss: 2.1988\n",
            "Epoch [3/10]      Batch [1300/1875] DLoss: 0.3457 GLoss: 1.6796\n",
            "Epoch [3/10]      Batch [1400/1875] DLoss: 0.2494 GLoss: 2.7561\n",
            "Epoch [3/10]      Batch [1500/1875] DLoss: 0.3659 GLoss: 2.6169\n",
            "Epoch [3/10]      Batch [1600/1875] DLoss: 0.2828 GLoss: 1.9961\n",
            "Epoch [3/10]      Batch [1700/1875] DLoss: 0.2012 GLoss: 2.4242\n",
            "Epoch [3/10]      Batch [1800/1875] DLoss: 0.5048 GLoss: 0.6494\n",
            "Epoch [4/10]      Batch [100/1875] DLoss: 0.4132 GLoss: 1.1200\n",
            "Epoch [4/10]      Batch [200/1875] DLoss: 0.1927 GLoss: 2.0596\n",
            "Epoch [4/10]      Batch [300/1875] DLoss: 0.3251 GLoss: 2.0516\n",
            "Epoch [4/10]      Batch [400/1875] DLoss: 0.2933 GLoss: 3.1443\n",
            "Epoch [4/10]      Batch [500/1875] DLoss: 0.2024 GLoss: 1.7582\n",
            "Epoch [4/10]      Batch [600/1875] DLoss: 0.1723 GLoss: 2.7460\n",
            "Epoch [4/10]      Batch [700/1875] DLoss: 0.4276 GLoss: 2.3010\n",
            "Epoch [4/10]      Batch [800/1875] DLoss: 0.1206 GLoss: 2.5132\n",
            "Epoch [4/10]      Batch [900/1875] DLoss: 0.1536 GLoss: 2.3083\n",
            "Epoch [4/10]      Batch [1000/1875] DLoss: 0.3212 GLoss: 1.6111\n",
            "Epoch [4/10]      Batch [1100/1875] DLoss: 0.0934 GLoss: 2.1962\n",
            "Epoch [4/10]      Batch [1200/1875] DLoss: 0.1368 GLoss: 2.1124\n",
            "Epoch [4/10]      Batch [1300/1875] DLoss: 0.3728 GLoss: 2.2455\n",
            "Epoch [4/10]      Batch [1400/1875] DLoss: 0.4988 GLoss: 1.1157\n",
            "Epoch [4/10]      Batch [1500/1875] DLoss: 0.5360 GLoss: 0.8630\n",
            "Epoch [4/10]      Batch [1600/1875] DLoss: 0.2726 GLoss: 2.0133\n",
            "Epoch [4/10]      Batch [1700/1875] DLoss: 0.3405 GLoss: 1.6708\n",
            "Epoch [4/10]      Batch [1800/1875] DLoss: 0.1031 GLoss: 2.5748\n",
            "Epoch [5/10]      Batch [100/1875] DLoss: 0.2215 GLoss: 3.0504\n",
            "Epoch [5/10]      Batch [200/1875] DLoss: 0.2891 GLoss: 1.8187\n",
            "Epoch [5/10]      Batch [300/1875] DLoss: 0.2714 GLoss: 1.3416\n",
            "Epoch [5/10]      Batch [400/1875] DLoss: 0.2465 GLoss: 3.5048\n",
            "Epoch [5/10]      Batch [500/1875] DLoss: 0.1708 GLoss: 2.1477\n",
            "Epoch [5/10]      Batch [600/1875] DLoss: 0.1512 GLoss: 2.1495\n",
            "Epoch [5/10]      Batch [700/1875] DLoss: 0.1541 GLoss: 2.6746\n",
            "Epoch [5/10]      Batch [800/1875] DLoss: 0.7827 GLoss: 0.4893\n",
            "Epoch [5/10]      Batch [900/1875] DLoss: 0.3023 GLoss: 1.9632\n",
            "Epoch [5/10]      Batch [1000/1875] DLoss: 0.2257 GLoss: 2.4134\n",
            "Epoch [5/10]      Batch [1100/1875] DLoss: 0.4838 GLoss: 0.7438\n",
            "Epoch [5/10]      Batch [1200/1875] DLoss: 0.2060 GLoss: 1.7709\n",
            "Epoch [5/10]      Batch [1300/1875] DLoss: 0.2375 GLoss: 2.0409\n",
            "Epoch [5/10]      Batch [1400/1875] DLoss: 0.7851 GLoss: 1.7084\n",
            "Epoch [5/10]      Batch [1500/1875] DLoss: 0.2154 GLoss: 1.9014\n",
            "Epoch [5/10]      Batch [1600/1875] DLoss: 0.1579 GLoss: 2.5129\n",
            "Epoch [5/10]      Batch [1700/1875] DLoss: 0.3242 GLoss: 2.3873\n",
            "Epoch [5/10]      Batch [1800/1875] DLoss: 0.1031 GLoss: 2.5759\n",
            "Epoch [6/10]      Batch [100/1875] DLoss: 0.3814 GLoss: 3.2331\n",
            "Epoch [6/10]      Batch [200/1875] DLoss: 0.3802 GLoss: 0.9491\n",
            "Epoch [6/10]      Batch [300/1875] DLoss: 0.1459 GLoss: 2.7438\n",
            "Epoch [6/10]      Batch [400/1875] DLoss: 0.4080 GLoss: 4.4280\n",
            "Epoch [6/10]      Batch [500/1875] DLoss: 0.0485 GLoss: 3.5649\n",
            "Epoch [6/10]      Batch [600/1875] DLoss: 0.1747 GLoss: 2.4584\n",
            "Epoch [6/10]      Batch [700/1875] DLoss: 0.2471 GLoss: 2.0250\n",
            "Epoch [6/10]      Batch [800/1875] DLoss: 0.8062 GLoss: 0.5557\n",
            "Epoch [6/10]      Batch [900/1875] DLoss: 0.1654 GLoss: 3.3669\n",
            "Epoch [6/10]      Batch [1000/1875] DLoss: 0.2666 GLoss: 1.9810\n",
            "Epoch [6/10]      Batch [1100/1875] DLoss: 0.1484 GLoss: 1.8971\n",
            "Epoch [6/10]      Batch [1200/1875] DLoss: 0.2253 GLoss: 1.7082\n",
            "Epoch [6/10]      Batch [1300/1875] DLoss: 0.1268 GLoss: 2.5986\n",
            "Epoch [6/10]      Batch [1400/1875] DLoss: 0.2377 GLoss: 2.5947\n",
            "Epoch [6/10]      Batch [1500/1875] DLoss: 0.1719 GLoss: 2.0812\n",
            "Epoch [6/10]      Batch [1600/1875] DLoss: 0.4901 GLoss: 3.5541\n",
            "Epoch [6/10]      Batch [1700/1875] DLoss: 0.4987 GLoss: 1.4624\n",
            "Epoch [6/10]      Batch [1800/1875] DLoss: 0.2780 GLoss: 1.8203\n",
            "Epoch [7/10]      Batch [100/1875] DLoss: 0.2708 GLoss: 3.8211\n",
            "Epoch [7/10]      Batch [200/1875] DLoss: 0.2366 GLoss: 1.8172\n",
            "Epoch [7/10]      Batch [300/1875] DLoss: 0.4498 GLoss: 2.2904\n",
            "Epoch [7/10]      Batch [400/1875] DLoss: 0.1314 GLoss: 4.8693\n",
            "Epoch [7/10]      Batch [500/1875] DLoss: 0.4489 GLoss: 1.1186\n",
            "Epoch [7/10]      Batch [600/1875] DLoss: 0.1217 GLoss: 3.4577\n",
            "Epoch [7/10]      Batch [700/1875] DLoss: 0.4648 GLoss: 2.1403\n",
            "Epoch [7/10]      Batch [800/1875] DLoss: 0.0392 GLoss: 4.6125\n",
            "Epoch [7/10]      Batch [900/1875] DLoss: 0.1329 GLoss: 2.9801\n",
            "Epoch [7/10]      Batch [1000/1875] DLoss: 0.1379 GLoss: 2.0574\n",
            "Epoch [7/10]      Batch [1100/1875] DLoss: 0.2345 GLoss: 1.3884\n",
            "Epoch [7/10]      Batch [1200/1875] DLoss: 0.5560 GLoss: 1.0336\n",
            "Epoch [7/10]      Batch [1300/1875] DLoss: 0.2596 GLoss: 4.1640\n",
            "Epoch [7/10]      Batch [1400/1875] DLoss: 0.2919 GLoss: 1.4395\n",
            "Epoch [7/10]      Batch [1500/1875] DLoss: 0.1875 GLoss: 2.3718\n",
            "Epoch [7/10]      Batch [1600/1875] DLoss: 0.9072 GLoss: 4.3457\n",
            "Epoch [7/10]      Batch [1700/1875] DLoss: 0.5510 GLoss: 2.2677\n",
            "Epoch [7/10]      Batch [1800/1875] DLoss: 0.4074 GLoss: 0.9579\n",
            "Epoch [8/10]      Batch [100/1875] DLoss: 0.4880 GLoss: 1.6741\n",
            "Epoch [8/10]      Batch [200/1875] DLoss: 0.1465 GLoss: 3.8928\n",
            "Epoch [8/10]      Batch [300/1875] DLoss: 0.1799 GLoss: 2.3233\n",
            "Epoch [8/10]      Batch [400/1875] DLoss: 0.2931 GLoss: 3.5583\n",
            "Epoch [8/10]      Batch [500/1875] DLoss: 0.1977 GLoss: 2.0133\n",
            "Epoch [8/10]      Batch [600/1875] DLoss: 0.1745 GLoss: 3.9191\n",
            "Epoch [8/10]      Batch [700/1875] DLoss: 0.3084 GLoss: 4.4639\n",
            "Epoch [8/10]      Batch [800/1875] DLoss: 0.1333 GLoss: 2.4500\n",
            "Epoch [8/10]      Batch [900/1875] DLoss: 0.0708 GLoss: 3.6140\n",
            "Epoch [8/10]      Batch [1000/1875] DLoss: 0.1031 GLoss: 2.4768\n",
            "Epoch [8/10]      Batch [1100/1875] DLoss: 0.1125 GLoss: 2.3356\n",
            "Epoch [8/10]      Batch [1200/1875] DLoss: 0.3068 GLoss: 1.4105\n",
            "Epoch [8/10]      Batch [1300/1875] DLoss: 0.3329 GLoss: 1.2245\n",
            "Epoch [8/10]      Batch [1400/1875] DLoss: 0.1551 GLoss: 2.6362\n",
            "Epoch [8/10]      Batch [1500/1875] DLoss: 0.2039 GLoss: 2.1532\n",
            "Epoch [8/10]      Batch [1600/1875] DLoss: 0.3729 GLoss: 2.8179\n",
            "Epoch [8/10]      Batch [1700/1875] DLoss: 0.2071 GLoss: 2.4154\n",
            "Epoch [8/10]      Batch [1800/1875] DLoss: 0.1501 GLoss: 2.3255\n",
            "Epoch [9/10]      Batch [100/1875] DLoss: 0.3522 GLoss: 3.1492\n",
            "Epoch [9/10]      Batch [200/1875] DLoss: 0.1541 GLoss: 4.7958\n",
            "Epoch [9/10]      Batch [300/1875] DLoss: 0.2279 GLoss: 1.8105\n",
            "Epoch [9/10]      Batch [400/1875] DLoss: 0.2088 GLoss: 2.9884\n",
            "Epoch [9/10]      Batch [500/1875] DLoss: 0.9283 GLoss: 0.3889\n",
            "Epoch [9/10]      Batch [600/1875] DLoss: 0.5225 GLoss: 1.1969\n",
            "Epoch [9/10]      Batch [700/1875] DLoss: 0.1780 GLoss: 3.0875\n",
            "Epoch [9/10]      Batch [800/1875] DLoss: 0.0833 GLoss: 3.4556\n",
            "Epoch [9/10]      Batch [900/1875] DLoss: 0.1905 GLoss: 2.2648\n",
            "Epoch [9/10]      Batch [1000/1875] DLoss: 0.1894 GLoss: 3.2426\n",
            "Epoch [9/10]      Batch [1100/1875] DLoss: 0.0551 GLoss: 4.0949\n",
            "Epoch [9/10]      Batch [1200/1875] DLoss: 0.0914 GLoss: 3.1737\n",
            "Epoch [9/10]      Batch [1300/1875] DLoss: 0.2466 GLoss: 2.0258\n",
            "Epoch [9/10]      Batch [1400/1875] DLoss: 0.1603 GLoss: 4.7137\n",
            "Epoch [9/10]      Batch [1500/1875] DLoss: 0.0624 GLoss: 3.2975\n",
            "Epoch [9/10]      Batch [1600/1875] DLoss: 1.0325 GLoss: 4.6218\n",
            "Epoch [9/10]      Batch [1700/1875] DLoss: 0.2236 GLoss: 3.8278\n",
            "Epoch [9/10]      Batch [1800/1875] DLoss: 0.1719 GLoss: 3.0125\n",
            "Epoch [10/10]      Batch [100/1875] DLoss: 0.1504 GLoss: 2.9589\n",
            "Epoch [10/10]      Batch [200/1875] DLoss: 0.4392 GLoss: 4.5262\n",
            "Epoch [10/10]      Batch [300/1875] DLoss: 0.2833 GLoss: 4.1666\n",
            "Epoch [10/10]      Batch [400/1875] DLoss: 0.2605 GLoss: 4.9081\n",
            "Epoch [10/10]      Batch [500/1875] DLoss: 0.0740 GLoss: 3.7143\n",
            "Epoch [10/10]      Batch [600/1875] DLoss: 0.4265 GLoss: 3.0268\n",
            "Epoch [10/10]      Batch [700/1875] DLoss: 0.1787 GLoss: 3.8005\n",
            "Epoch [10/10]      Batch [800/1875] DLoss: 0.1421 GLoss: 2.3163\n",
            "Epoch [10/10]      Batch [900/1875] DLoss: 0.2143 GLoss: 3.5532\n",
            "Epoch [10/10]      Batch [1000/1875] DLoss: 0.7051 GLoss: 0.5762\n",
            "Epoch [10/10]      Batch [1100/1875] DLoss: 0.1257 GLoss: 4.0488\n",
            "Epoch [10/10]      Batch [1200/1875] DLoss: 0.2255 GLoss: 1.3300\n",
            "Epoch [10/10]      Batch [1300/1875] DLoss: 0.5588 GLoss: 0.7273\n",
            "Epoch [10/10]      Batch [1400/1875] DLoss: 0.1779 GLoss: 2.1470\n",
            "Epoch [10/10]      Batch [1500/1875] DLoss: 0.3384 GLoss: 1.0670\n",
            "Epoch [10/10]      Batch [1600/1875] DLoss: 0.1075 GLoss: 3.4565\n",
            "Epoch [10/10]      Batch [1700/1875] DLoss: 0.4384 GLoss: 3.8643\n",
            "Epoch [10/10]      Batch [1800/1875] DLoss: 0.0395 GLoss: 5.6514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  z = tf.random.normal((9, latent_dim))\n",
        "  generated = generator.predict(z)\n",
        "  print(generated.shape)\n",
        "  grid = torchvision.utils.make_grid(np.transpose(torch.tensor(generated), (0, 3, 2, 1)),nrow=3, normalize=True)\n",
        "  print(grid.shape)\n",
        "\n",
        "  plt.imshow(np.transpose(grid, (2, 1, 0)))\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "rJAwvieHiua1",
        "outputId": "eac529a1-8bec-4af8-a3e7-d8c08647f8f0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 53ms/step\n",
            "(9, 28, 28, 1)\n",
            "torch.Size([3, 92, 92])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator.save('FashionMNISTGenerator.keras')"
      ],
      "metadata": {
        "id": "s1dLK8HMBR2g"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.save('FashionMNISTDiscriminator.keras')"
      ],
      "metadata": {
        "id": "JYZBJgBHdVUO"
      },
      "execution_count": 58,
      "outputs": []
    }
  ]
}